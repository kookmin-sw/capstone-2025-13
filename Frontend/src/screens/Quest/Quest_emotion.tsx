import { StyleSheet, Text, View, Image } from 'react-native';
import { useEffect, useState, useRef } from 'react';
import { Camera, useCameraDevice, useFrameProcessor } from 'react-native-vision-camera';
import { Face, useFaceDetector } from 'react-native-vision-camera-face-detector';
import { Worklets } from 'react-native-worklets-core';

import { useLoadEmotionModel } from '../../hooks/useLoadEmotionModel';
import { shouldCaptureFace } from '../../utils/faceChecker';
import { EmotionModelRunner } from '../../utils/EmotionModelRun';
import { QUESTS } from '../../utils/QuestEmotion/quests';

import EmotionChartBox from '../../components/Quest_emotionBox';
import styles from '../../styles/questEmotionStyles';

export default function QuestEmotion() {
  const [emotionLog, setEmotionLog] = useState<string[]>([]);
  const device = useCameraDevice('front');
  const cameraRef = useRef<any>(null);
  const { detectFaces } = useFaceDetector();
  const [hasPermission, setHasPermission] = useState(false);
  const { isLoaded, model } = useLoadEmotionModel();
  const [noFaceWarning, setNoFaceWarning] = useState(false);

  const [photoPath, setPhotoPath] = useState<string | null>(null);
  const [latestResult, setLatestResult] = useState<number[] | null>(null);
const [success, setSuccess] = useState<boolean>(false);
  // íƒ€ì´ë° ì œì–´
  const lastPhotoTimeRef = useRef(0);
  const isPhotoTaken = useRef(false);
  
  const quest_name = '5ì´ˆ ê°„ ì›ƒì–´ë³´ê¸°';
  const quest = QUESTS.find(q => q.id === quest_name);
  if (!quest) {
    return (
      <View style={styles.centered}>
        <Text>âŒ í€˜ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤</Text>
      </View>
    );
  }

  const quest_capture_interval = quest.interval ?? 1000;
  const quest_save_pre_log = quest.logLength ?? 20;

  const capturePhoto = async (face: Face | undefined) => {
    if (isPhotoTaken.current) return null;
    isPhotoTaken.current = true;

    const now = Date.now();
    const { isLargeEnough, now: checkedTime } = shouldCaptureFace(face, lastPhotoTimeRef.current);
    if (!isLargeEnough || now - lastPhotoTimeRef.current < quest_capture_interval) {
      isPhotoTaken.current = false;
      return null;
    }

    try {
      const photo = await cameraRef.current.takePhoto();
      const path = `file://${photo.path}`;
      setPhotoPath(path);
      console.log('ğŸ“¸ ì‚¬ì§„ ì €ì¥ë¨:', path);
      lastPhotoTimeRef.current = checkedTime;
      return path;
    } catch (err) {
      console.error('âŒ ì‚¬ì§„ ìº¡ì²˜ ì‹¤íŒ¨:', err);
      return null;
    } finally {
      isPhotoTaken.current = false;
      console.log('ğŸ”„ isPhotoTaken reset');
    }
  };

const handleDetectedFaces = Worklets.createRunOnJS(async (faces: Face[]) => {
  // faces ë°°ì—´ì´ ë“¤ì–´ì˜¤ê¸´ í–ˆì§€ë§Œ ê¸¸ì´ê°€ 0ì´ë©´ ê²½ê³  ì¼œê³  ë¦¬í„´
  if (faces && faces.length === 0) {
    setNoFaceWarning(true);
    return;
  }
  // ì–¼êµ´ì´ í•˜ë‚˜ë¼ë„ ì¡íˆë©´ ê²½ê³  ë„ê¸°
  setNoFaceWarning(false);

  // ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ ì²´í¬
  if (!faces?.length || !isLoaded || !model) return;

  const face = faces[0];
  const uri = await capturePhoto(face);
  if (!uri) return;

  const result = await EmotionModelRunner(uri, model);
  if (result) {
    const labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral'];
    const topIndex = result.indexOf(Math.max(...result));
    const predictedLabel = labels[topIndex];
    const updated = [...emotionLog, predictedLabel];

    setLatestResult(Array.from(result));
    console.log('Predicted Label:', predictedLabel);

    if (updated.length > quest_save_pre_log) updated.shift();
    setEmotionLog(updated);

    if (quest.check(updated)) {
      setSuccess(true);
      console.log('ğŸ¯ í€˜ìŠ¤íŠ¸ ì™„ë£Œ');
    }
  }
});


  const frameProcessor = useFrameProcessor((frame) => {
    "worklet";
    const now = Date.now();
    const last = (globalThis as any).lastProcessTime ?? 0;
    if (now - last < quest_capture_interval) return;
    (globalThis as any).lastProcessTime = now;

    const faces = detectFaces(frame);
    handleDetectedFaces(faces);
  }, [handleDetectedFaces]);

  useEffect(() => {
    (async () => {
      const status = await Camera.requestCameraPermission();
      setHasPermission(status === 'granted');
    })();
  }, []);

  if (!device || !hasPermission) {
    return (
        <View style={styles.centered}>
          <Text>ğŸ“· ì¹´ë©”ë¼ ì¤€ë¹„ ì¤‘...</Text>
        </View>
      );
    }

  return (
    <View style={styles.container}>
  <View style={[styles.half, { flex: 7 }]}>
    <Camera
      ref={cameraRef}
      style={styles.camera}
      device={device}
      photo
      isActive
      frameProcessor={frameProcessor}
    />
  </View>

    {latestResult !== null ? (
      <View style={styles.overlay}>
        <EmotionChartBox result={latestResult} success= {success} />
      </View>
    ) : (
      <View style={styles.overlay}>
        <View style={styles.centered}>
          <Text style={styles.warningText}>âš ï¸ ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤</Text>
        </View>
      </View>
    )}
</View>
  );
}
